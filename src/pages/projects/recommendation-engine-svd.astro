---
import Layout from '../../layouts/Layout.astro';
---
<style>
    /* Basic styling for code blocks */
    pre {
        background-color: #1f2937; /* gray-800 */
        color: #d1d5db; /* gray-300 */
        padding: 1rem;
        border-radius: 0.5rem;
        overflow-x: auto;
        font-family: 'Courier New', Courier, monospace;
        font-size: 0.9em;
    }
    pre code .token.comment {
        color: #9ca3af; /* gray-400 */
    }
    pre code .token.keyword {
        color: #f9a8d4; /* pink-300 */
    }
    pre code .token.function {
        color: #a7f3d0; /* emerald-200 */
    }
    pre code .token.string {
        color: #fde047; /* yellow-300 */
    }
     pre code .token.number {
        color: #d8b4fe; /* purple-300 */
    }
    pre code .token.operator {
        color: #f9a8d4; /* pink-300 */
    }
</style>

<Layout title="Project Showcase: Recommendation Engine" description="How Matrix Factorization Solved the Kindle Discovery Problem">

    <div class="container mx-auto max-w-4xl px-4 sm:px-6 lg:px-8 py-12 md:py-20">
        
        <article class="bg-white rounded-xl shadow-lg overflow-hidden">
            <header class="p-8 md:p-12 text-center bg-gray-100 border-b border-gray-200">
                <h1 class="text-3xl md:text-5xl font-bold text-gray-900 leading-tight">Project Showcase: Building a High-Accuracy Recommendation Engine</h1>
                <p class="mt-4 text-lg md:text-xl text-indigo-600 font-medium">How Matrix Factorization Solved the Kindle Discovery Problem</p>
            </header>

            <div class="p-8 md:p-12 space-y-12">
                <!-- Section 1: The Business Problem -->
                <section>
                    <h2 class="text-2xl md:text-3xl font-bold text-gray-900 mb-4 flex items-center"><span class="mr-3 text-3xl">üéØ</span>The Business Problem: Discovery Fatigue</h2>
                    <div class="prose max-w-none text-lg text-gray-700 space-y-4">
                        <p>In digital retail, choice is overwhelming. For a platform like Kindle, simply listing best-sellers isn't enough. True success comes from predicting what an individual user will love, turning browsing into buying.</p>
                        <p class="font-semibold bg-indigo-50 p-4 rounded-lg border border-indigo-200"><strong>Our Goal:</strong> Build a recommendation engine using the large-scale Book-Crossing dataset to deliver personalized recommendations, increase user engagement, and drive sales.</p>
                        <img src="https://placehold.co/800x400/6366f1/ffffff?text=User+Engagement" alt="Illustration of user engagement with a digital platform" class="rounded-lg shadow-md w-full mt-6" onerror="this.onerror=null;this.src='https://placehold.co/800x400/cccccc/ffffff?text=Image+Not+Found';">
                    </div>
                </section>

                <!-- Section 2: The Sparsity Defense -->
                <section>
                    <h2 class="text-2xl md:text-3xl font-bold text-gray-900 mb-4 flex items-center"><span class="mr-3 text-3xl">üßπ</span>Phase 1: The Sparsity Defense</h2>
                    <div class="prose max-w-none text-lg text-gray-700 space-y-4">
                        <p>Any large dataset has a core challenge: sparsity. Our initial dataset had over 1 million ratings, but most of the user-item matrix was empty space. A model can't learn from nothing.</p>
                        <div class="bg-gray-50 p-6 rounded-lg border border-gray-200">
                            <h3 class="font-semibold text-xl text-gray-800 mb-3">Our Strategy:</h3>
                            <p>We performed a crucial data engineering step to keep only high-quality interaction data:</p>
                            <ul class="list-disc list-inside mt-2 space-y-2">
                                <li><strong>Users Filtered:</strong> Kept only users who rated at least <strong>20 books</strong>.</li>
                                <li><strong>Books Filtered:</strong> Kept only books rated by at least <strong>10 users</strong>.</li>
                            </ul>
                            <p class="mt-4 font-bold text-green-700 bg-green-100 p-3 rounded-md">Result: This removed 65% of noisy data, creating a robust dataset of 359,054 high-quality ratings.</p>
                        </div>
                    </div>
                </section>

                <!-- Section 3: Why SVD is Superior -->
                <section>
                    <h2 class="text-2xl md:text-3xl font-bold text-gray-900 mb-4 flex items-center"><span class="mr-3 text-3xl">üí°</span>Phase 2: Why SVD is the Superior Choice</h2>
                    <div class="prose max-w-none text-lg text-gray-700 space-y-6">
                        <p>We tested two collaborative filtering methods, but one was the clear winner for this business problem.</p>
                        <div class="grid md:grid-cols-2 gap-6">
                            <div class="border border-red-300 bg-red-50 p-4 rounded-lg">
                                <h3 class="font-bold text-xl text-red-800">K-Nearest Neighbors (KNN) ‚ùå</h3>
                                <p class="mt-2">This "neighborhood" model relies on finding users who rated the same books. In a sparse dataset, these direct overlaps are rare. Too slow and inaccurate for our sparse data.</p>
                            </div>
                             <div class="border border-green-300 bg-green-50 p-4 rounded-lg">
                                <h3 class="font-bold text-xl text-green-800">Singular Value Decomposition (SVD) ‚úÖ</h3>
                                <p class="mt-2">This "model-based" approach uses matrix factorization to find hidden patterns (latent factors), even without direct overlaps.</p>
                            </div>
                        </div>
                        <p class="text-center font-mono text-xl p-4 bg-gray-100 rounded-md">The Core Idea: $R \approx P \times Q^T$</p>
                    </div>
                </section>

                <!-- Section 4: Latent Factors -->
                <section>
                    <h2 class="text-2xl md:text-3xl font-bold text-gray-900 mb-4 flex items-center"><span class="mr-3 text-3xl">üîÆ</span>Phase 3: Discovering "Latent Factors"</h2>
                    <div class="prose max-w-none text-lg text-gray-700 space-y-4">
                        <p>This is the mathematical magic behind the model. Latent factors are the hidden features SVD uncovers automatically. Think of them as the model learning abstract concepts on its own, like:</p>
                        <blockquote class="text-lg">
                            <p>"Preference for high-stakes thrillers"</p>
                            <p>"Dislike of slow-paced historical fiction"</p>
                        </blockquote>
                        <p><strong>Tuning for Perfection:</strong> The key is choosing the right number of factors. Using GridSearchCV, we determined the optimal number to be <strong>100</strong>. This allowed the model to capture complex user tastes without "overfitting" (memorizing noise).</p>
                    </div>
                </section>

                <!-- NEW SECTION: Technical Deep Dive -->
                <section>
                    <h2 class="text-2xl md:text-3xl font-bold text-gray-900 mb-4 flex items-center"><span class="mr-3 text-3xl">üíª</span>Technical Deep Dive: The Code</h2>
                    <div class="prose max-w-none text-lg text-gray-700 space-y-8">
                        <p>Here are the key Python snippets that powered this project, using the `pandas` and `surprise` libraries.</p>
                        
                        <div>
                            <h3 class="text-xl font-semibold mb-2">1. Data Filtering (The Sparsity Defense)</h3>
                            <pre><code class="language-python">import pandas as pd

# Load the datasets
users = pd.read_csv('BX-Users.csv', sep=';', on_bad_lines='skip', encoding='latin-1')
ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', on_bad_lines='skip', encoding='latin-1')

# Calculate rating counts for books and users
user_rating_counts = ratings['User-ID'].value_counts()
book_rating_counts = ratings['ISBN'].value_counts()

# Filter out users with less than 20 ratings
robust_users = user_rating_counts[user_rating_counts >= 20].index
ratings = ratings[ratings['User-ID'].isin(robust_users)]

# Filter out books with less than 10 ratings
robust_books = book_rating_counts[book_rating_counts >= 10].index
ratings = ratings[ratings['ISBN'].isin(robust_books)]

# Final cleaned ratings: 359,054 rows
print(f"Shape of the final, robust ratings dataset: {ratings.shape}")
</code></pre>
                        </div>

                        <div>
                            <h3 class="text-xl font-semibold mb-2">2. Model Training & Hyperparameter Tuning</h3>
                            <pre><code class="language-python">from surprise import Dataset, Reader, SVD
from surprise.model_selection import GridSearchCV

# The Reader is needed to parse the file
reader = Reader(rating_scale=(1, 10))

# Load the data into Surprise's format
data = Dataset.load_from_df(ratings[['User-ID', 'ISBN', 'Book-Rating']], reader)

# Define the parameter grid for GridSearchCV
# We focus on n_factors as it was the key hyperparameter
param_grid = {'n_factors': [50, 100, 150], 'n_epochs': [20], 'lr_all': [0.005], 'reg_all': [0.02]}
gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)

# Find the best model
gs.fit(data)

# Best RMSE score and parameters
print(f"Best RMSE score: {gs.best_score['rmse']:.4f}") # Should be ~3.4965
print(f"Best parameters: {gs.best_params['rmse']}") # n_factors should be 100

# Train the final model with the best parameters
final_model = gs.best_estimator['rmse']
trainset = data.build_full_trainset()
final_model.fit(trainset)
</code></pre>
                        </div>

                         <div>
                            <h3 class="text-xl font-semibold mb-2">3. Getting Predictions for a User</h3>
                            <pre><code class="language-python">def get_top_n_recommendations(user_id, n=10):
    # Get a list of all book ISBNs
    all_books = ratings['ISBN'].unique()
    
    # Get a list of books the user has already rated
    rated_books = ratings[ratings['User-ID'] == user_id]['ISBN']
    
    # Get books the user has NOT rated
    books_to_predict = [book for book in all_books if book not in rated_books]
    
    # Predict ratings for the unrated books
    predictions = [final_model.predict(user_id, book_isbn) for book_isbn in books_to_predict]
    
    # Sort predictions by estimated rating
    predictions.sort(key=lambda x: x.est, reverse=True)
    
    # Return the top N recommendations
    top_n = predictions[:n]
    
    # You would typically map ISBNs back to book titles here
    return [(pred.iid, pred.est) for pred in top_n]

# Example usage for a sample user
sample_user_id = 204622 # Example user from the dataset
recommendations = get_top_n_recommendations(sample_user_id, n=5)

print(f"Top 5 recommendations for user {sample_user_id}:")
for book_isbn, estimated_rating in recommendations:
    print(f"  - ISBN: {book_isbn}, Predicted Rating: {estimated_rating:.2f}")

</code></pre>
                        </div>
                    </div>
                </section>

                <!-- Section 5: Performance & Impact -->
                <section>
                    <h2 class="text-2xl md:text-3xl font-bold text-gray-900 mb-4 flex items-center"><span class="mr-3 text-3xl">üèÜ</span>Phase 4: Final Performance & Business Impact</h2>
                    <div class="prose max-w-none text-lg text-gray-700 space-y-6">
                        <p>The SVD model delivered the lowest prediction error, making it the clear choice for deployment.</p>
                        
                        <!-- Results Table -->
                        <div class="overflow-x-auto">
                            <table class="min-w-full divide-y divide-gray-200 border rounded-lg">
                                <thead class="bg-gray-50">
                                    <tr>
                                        <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Model</th>
                                        <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Average RMSE</th>
                                        <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">Final Decision</th>
                                    </tr>
                                </thead>
                                <tbody class="bg-white divide-y divide-gray-200">
                                    <tr>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">Normal Predictor (Baseline)</td>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">~3.7500</td>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm font-semibold text-red-600">‚ùå Rejected (Benchmark)</td>
                                    </tr>
                                    <tr>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">KNNWithMeans (Tuned)</td>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-500">3.5427</td>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm font-semibold text-red-600">‚ùå Rejected (High Error)</td>
                                    </tr>
                                    <tr class="bg-green-50">
                                        <td class="px-6 py-4 whitespace-nowrap text-sm font-medium text-gray-900">SVD (Tuned)</td>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm font-bold text-green-700">3.4965</td>
                                        <td class="px-6 py-4 whitespace-nowrap text-sm font-semibold text-green-700">‚úÖ Accepted - Final Model</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="bg-indigo-50 p-6 rounded-lg border border-indigo-200 mt-8">
                            <h3 class="font-bold text-xl text-indigo-800">The Business Win: RMSE = 3.4965</h3>
                            <p class="mt-2">This isn't just a number; it's a measure of reliability. Root Mean Squared Error (RMSE) heavily penalizes large mistakes.</p>
                            <ul class="list-disc list-inside mt-3 space-y-2">
                                <li><strong>Builds User Trust:</strong> A low RMSE ensures the model avoids egregious errors, which is essential for user retention.</li>
                                <li><strong>Drives Sales:</strong> Accurate, personalized recommendations reduce user fatigue and directly lift sales conversion rates.</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </div>
            
            <footer class="bg-gray-800 text-white p-8 md:p-12 text-center">
                <p class="text-lg">This project delivered a highly accurate, deployment-ready system designed to drive commercial growth by transforming sparse data into a powerful business asset.</p>
            </footer>
        </article>

    </div>

</Layout>